x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
5
train
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
train
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod,s=50,type="coefficients")[1:12,]
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select('price') %>% unlist() # %>% as.numeric()
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
mean((ridge.pred-y_test)^2)
mean((mean(y_train)-y_test)^2) # the test set MSE
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((ridge.pred-y_test)^2)
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out)
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:12,]
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(warning = F, results = F, message = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
# unload/detact package when done using it
# detach_package = function(pkg, character.only = FALSE) { if(!character.only) { pkg <- deparse(substitute(pkg)) } search_item <- paste("package", pkg, sep = ":") while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } }
uzscale <- function(df, append=0, excl=NULL) {
#' Standardize dataframe to z scores, safe for non-numeric variables.
#' ELo 201904 GWU DATS
#' @param df The dataframe.
#' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
#' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
#' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
#' @examples
#' library("ISLR")
#' tmp = uzscale( Hitters )
#' tmp = uzscale( Hitters, 1 )
#' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append
nmax = length(df)
if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
df1 = df
onames = colnames(df)  # the original column names
cnames = onames  # the new column names, if needed start with the original ones
znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
j=1  # counting index
for( i in 1:nmax ) {
if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) {
df1[,j+nadd] = scale(df[,i])
cnames = c(cnames, znames[i])
j=j+1
} else if ( !append ) { j=j+1
} # if append == 1 and (colunm non-numeric or excluded), do not advance j.
}
if (append) { colnames(df1) <- cnames }
return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
loadPkg("ISLR")
HittersClean = subset(Hitters, Salary != "NA")
HittersClean = uzscale(HittersClean)
colnames(Hitters)
x=model.matrix(Salary~.,HittersClean)[,-1]
y=HittersClean$Salary
HittersClean
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
predict(ridge.mod,s=50,type="coefficients")[1:20,]
loadPkg("dplyr")
set.seed(1)
train = HittersClean %>% sample_frac(0.5)
test = HittersClean %>% setdiff(train)
x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]
y_train = train %>% select(Salary) %>% unlist() # %>% as.numeric()
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
train
ols.mod = lm(Salary~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
train
ols.mod = lm(price~., data = train)
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((mean(y_train)-y_test)^2) # the test set MSE
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
house_unscale
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
train
ols.mod = lm(price~., data = train)
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
loadPkg("readr")
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg('class')
kc_house_data <- data.frame(read_csv("kc_house_data.csv"))
kc_house_data <- subset(kc_house_data, select = -c(9, 10))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
kc_house_data$condition <- as.factor(kc_house_data$condition)
kc_house_data$grade <- as.factor(kc_house_data$grade)
kc_house_data$price <- cut(kc_house_data$price, breaks = 3, labels = c("Low", "Medium", "High")) # categorization of house prices
summary(kc_house_data$price) # distribution of data points according to their categories
set.seed(1)
kc_house_data_train_rows = sample(1:nrow(kc_house_data),     #<- from 1 to the number of
#   rows in the data set
round(0.8 * nrow(kc_house_data), 0),  #<- multiply the
#   number of rows
#   by 0.8 and round
#   the decimals
replace = FALSE)       #<- don't replace the numbers
# Let's check to make sure we have 80% of the rows.
length(kc_house_data_train_rows) / nrow(kc_house_data)
kc_house_data_train = kc_house_data[kc_house_data_train_rows, ]  #<- select the rows identified in
#   the kc_house_data_train_rows data
kc_house_data_test = kc_house_data[-kc_house_data_train_rows, ]  #<- select the rows that weren't
#   identified in the
#   kc_house_data_train_rows data
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
set.seed(1)
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k,                #<- number of neighbors considered
use.all = TRUE)       #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 25, by = 2),  #<- set k to be odd number from 1 to 25
function(x) chooseK(x,
train_set = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
val_set = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
train_class = kc_house_data_train[, "price"],
val_class = kc_house_data_test[, "price"]))
# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
accuracy = knn_different_k[2,])
# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")
ggplot(knn_different_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3)
# Let's train the classifier for k = 5
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class")
loadPkg("class")
# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
price_predict = knn(train = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],  #<- training set cases
test = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],    #<- test set cases
cl = kc_house_data_train[, "price"],                         #<- category for true classification
k = 5) #,                                                    #<- number of neighbors considered
# use.all = TRUE)                                            #<- control ties between class assignments
#   If true, all distances equal to the kth
#   largest are included
# View the output.
str(price_predict)
length(price_predict)
table(price_predict)
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the
# predictions from price_predict to the original data set.
kNN_res = table(price_predict,
kc_house_data_test$`price`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples
# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]
# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("rpart")
loadPkg("caret")
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
loadPkg("tree")
loadPkg('ISLR')
loadPkg('randomForest')
loadPkg('leaps')
kc_house_data <- read.csv("kc_house_data.csv")
str(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(9, 10))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
kc_house_data$condition <- as.factor(kc_house_data$condition)
kc_house_data$grade <- as.factor(kc_house_data$grade)
price.ln = log(kc_house_data$price)
tree1 <- tree(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data)
summary(tree1)
plot(tree1)
text(tree1,cex=0.75, digits=3)
tree2 <- rpart(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data, cp=.02)
par(xpd = NA, mar = rep(0.7, 4))
plot(tree2, compress = TRUE)
text(tree2, cex = 0.7, use.n = TRUE, fancy = TRUE, all = TRUE)
fancyRpartPlot(tree2)
rpart.plot(tree2, box.palette="RdBu", shadow.col="gray", nn=TRUE)
treefit = tree(log(price) ~ long+lat,data=kc_house_data)
plot(treefit)
text(treefit, cex=0.75, digits = 3)
price.deciles = quantile(kc_house_data$price,0:10/10)
cut.prices = cut(kc_house_data$price,price.deciles,include.lowest=TRUE)
plot(kc_house_data$long,kc_house_data$lat,col=grey(10:2/11)[cut.prices],pch=20,xlab="Longitude",ylab="Latitude")
partition.tree(treefit,ordvars=c("long","lat"),add=TRUE, col = 'red')
tree1.seq <- prune.tree(tree1) # Sequence of pruned tree sizes/errors
plot(tree1.seq)  # error versus plot size
tree1.seq$dev # Vector of error rates for prunings in order
opt.trees = which(tree1.seq$dev == min(tree1.seq$dev)) # Positions of optimal (with respect to error) trees
min(tree1.seq$size[opt.trees]) # Size of smallest optimal tree
prune.tree(tree1,best=5) # Returns best pruned tree with all data
fold <- floor(runif(nrow(kc_house_data),1,11))
kc_house_data$fold <- fold
test.set <- kc_house_data[kc_house_data$fold == 1,]
train.set <- kc_house_data[kc_house_data$fold != 1,]
tree.pred <- tree(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set, mindev=0.001)
treepr <- prune.tree(tree.pred, best = 5) # Return best pruned tree with 5 leaves, evaluating error on training data
treepr
plot(treepr)
text(treepr,cex=0.75, digits=3)
tree.pred.seq <- prune.tree(tree.pred)
plot(tree.pred.seq)
tree.pred.seq$dev
opt.trees = which(tree.pred.seq$dev == min(tree.pred.seq$dev))
min(tree.pred.seq$size[opt.trees])
treepr.pred <- prune.tree(tree.pred, best = 5, newdata = test.set) #evaluates on test data
treepr.pred
plot(treepr.pred)
text(treepr.pred,cex=0.75, digits=3)
ff1 <- randomForest(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data = train.set, importance = TRUE)
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("glmnet")
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
# kc_house_data$condition <- as.factor(kc_house_data$condition)
# kc_house_data$grade <- as.factor(kc_house_data$grade)
colnames(kc_house_data)
nrow(kc_house_data)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
house_unscale
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
predict(ridge.mod,s=50,type="coefficients")[1:12,]
predict(ridge.mod,s=50,type="coefficients")[1:12,]
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
mean((ridge.pred-y_test)^2)
mean((mean(y_train)-y_test)^2) # the test set MSE
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((ridge.pred-y_test)^2)
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:12,]
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((ridge.pred - y_test)^2)
rsq <- 1 - sse / sst
rsq
rsq
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:12,]
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((ridge.pred - y_test)^2)
rsq <- 1 - sse / sst
rsq
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
rsq1
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
