test.set <- kc_house_data[kc_house_data$fold == 1,]
train.set <- kc_house_data[kc_house_data$fold != 1,]
tree.pred <- tree(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set, mindev=0.001)
treepr <- prune.tree(tree.pred, best = 5) # Return best pruned tree with 5 leaves, evaluating error on training data
treepr
plot(treepr)
text(treepr,cex=0.75, digits=3)
tree.pred.seq <- prune.tree(tree.pred)
plot(tree.pred.seq)
tree.pred.seq$dev
opt.trees = which(tree.pred.seq$dev == min(tree.pred.seq$dev))
min(tree.pred.seq$size[opt.trees])
treepr.pred <- prune.tree(tree.pred, best = 5, newdata = test.set) #evaluates on test data
treepr.pred
plot(treepr.pred)
text(treepr.pred,cex=0.75, digits=3)
ff1 <- randomForest(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data = train.set, importance = TRUE)
ff1
library("dplyr")
library("tidyr")
library("tidyverse")
library("ggplot2")
library("Rmisc")
library("MASS")
library("corrplot")
library("faraway")
library("factoextra")
library("pls")
library("magrittr")
house <- data.frame(read.csv("kc_house_data.csv", header = TRUE))
str(house)
house2 <- subset(house,select = -c(id,date,waterfront,view,zipcode,lat,long,sqft_living15,sqft_lot15,yr_renovated))
str(house2)
sum(is.na(house2))
corx=cor(house2[,-1])
corx
prin_comp <- prcomp(scale(house2[,-1]))
summary(prin_comp)
prin_comp$center ##output the mean of variables
prin_comp$scale ##output the sd of variables
prin_comp$rotation
fviz_eig(prin_comp)
biplot(prin_comp, scale = 0)
##The first principle component looks like sqft_above, the second looks like sqft_basement)
biplot(prin_comp,6:7, scale =0)
pr.var <- (prin_comp$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
houseprice <- prin_comp$x
modHouses <- lm(house2$price ~ houseprice[,1:7])
summary(modHouses)
fullmodel=lm(price~.-sqft_basement,data=house2)
summary(fullmodel)
par(mfrow = c(1,2))
plot(house2$price, predict(modHouses), xlab = "actual price", ylab = "Predicted price", main = "PCR", abline(a = 0, b = 1, col = "red"))
plot(house2$price, predict(fullmodel), xlab = "actual price", ylab = "Predicted price", main = "Full model", abline(a = 0, b = 1, col = "red"))
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("glmnet")
uzscale <- function(df, append=0, excl=NULL) {
#' Standardize dataframe to z scores, safe for non-numeric variables.
#' ELo 201904 GWU DATS
#' @param df The dataframe.
#' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
#' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
#' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
#' @examples
#' library("ISLR")
#' tmp = uzscale( Hitters )
#' tmp = uzscale( Hitters, 1 )
#' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append
nmax = length(df)
if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
df1 = df
onames = colnames(df)  # the original column names
cnames = onames  # the new column names, if needed start with the original ones
znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
j=1  # counting index
for( i in 1:nmax ) {
if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) {
df1[,j+nadd] = scale(df[,i])
cnames = c(cnames, znames[i])
j=j+1
} else if ( !append ) { j=j+1
} # if append == 1 and (colunm non-numeric or excluded), do not advance j.
}
if (append) { colnames(df1) <- cnames }
return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
# kc_house_data$condition <- as.factor(kc_house_data$condition)
# kc_house_data$grade <- as.factor(kc_house_data$grade)
colnames(kc_house_data)
nrow(kc_house_data)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
house_unscale
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
predict(ridge.mod,s=50,type="coefficients")[1:12,]
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
5
train
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
train
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod,s=50,type="coefficients")[1:12,]
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select(price) %>% unlist() # %>% as.numeric()
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% select('price') %>% unlist() # %>% as.numeric()
loadPkg("dplyr")
house_unscale
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
mean((ridge.pred-y_test)^2)
mean((mean(y_train)-y_test)^2) # the test set MSE
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((ridge.pred-y_test)^2)
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out)
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:12,]
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(warning = F, results = F, message = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
# unload/detact package when done using it
# detach_package = function(pkg, character.only = FALSE) { if(!character.only) { pkg <- deparse(substitute(pkg)) } search_item <- paste("package", pkg, sep = ":") while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } }
uzscale <- function(df, append=0, excl=NULL) {
#' Standardize dataframe to z scores, safe for non-numeric variables.
#' ELo 201904 GWU DATS
#' @param df The dataframe.
#' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
#' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
#' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
#' @examples
#' library("ISLR")
#' tmp = uzscale( Hitters )
#' tmp = uzscale( Hitters, 1 )
#' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append
nmax = length(df)
if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
df1 = df
onames = colnames(df)  # the original column names
cnames = onames  # the new column names, if needed start with the original ones
znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
j=1  # counting index
for( i in 1:nmax ) {
if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) {
df1[,j+nadd] = scale(df[,i])
cnames = c(cnames, znames[i])
j=j+1
} else if ( !append ) { j=j+1
} # if append == 1 and (colunm non-numeric or excluded), do not advance j.
}
if (append) { colnames(df1) <- cnames }
return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
loadPkg("ISLR")
HittersClean = subset(Hitters, Salary != "NA")
HittersClean = uzscale(HittersClean)
colnames(Hitters)
x=model.matrix(Salary~.,HittersClean)[,-1]
y=HittersClean$Salary
HittersClean
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
predict(ridge.mod,s=50,type="coefficients")[1:20,]
loadPkg("dplyr")
set.seed(1)
train = HittersClean %>% sample_frac(0.5)
test = HittersClean %>% setdiff(train)
x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]
y_train = train %>% select(Salary) %>% unlist() # %>% as.numeric()
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
train
ols.mod = lm(Salary~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
house_unscale
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("glmnet")
uzscale <- function(df, append=0, excl=NULL) {
#' Standardize dataframe to z scores, safe for non-numeric variables.
#' ELo 201904 GWU DATS
#' @param df The dataframe.
#' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
#' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
#' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
#' @examples
#' library("ISLR")
#' tmp = uzscale( Hitters )
#' tmp = uzscale( Hitters, 1 )
#' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append
nmax = length(df)
if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
df1 = df
onames = colnames(df)  # the original column names
cnames = onames  # the new column names, if needed start with the original ones
znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
j=1  # counting index
for( i in 1:nmax ) {
if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) {
df1[,j+nadd] = scale(df[,i])
cnames = c(cnames, znames[i])
j=j+1
} else if ( !append ) { j=j+1
} # if append == 1 and (colunm non-numeric or excluded), do not advance j.
}
if (append) { colnames(df1) <- cnames }
return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
# kc_house_data$condition <- as.factor(kc_house_data$condition)
# kc_house_data$grade <- as.factor(kc_house_data$grade)
colnames(kc_house_data)
nrow(kc_house_data)
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
predict(ridge.mod,s=50,type="coefficients")[1:12,]
loadPkg("dplyr")
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)
x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]
y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
mean((ridge.pred-y_test)^2)
mean((mean(y_train)-y_test)^2) # the test set MSE
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((ridge.pred-y_test)^2)
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out)
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:12,]
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((ridge.pred - y_test)^2)
rsq <- 1 - sse / sst
rsq
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
rsq1
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
