---
title: "Housing Market - Paper2"
author: "Anti-Code Group"
date: "11/26/2019"
output:  
    rmdformats::readthedown:
      toc_float: true
      number_sections: true
      includes:
        before_body: header.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999,  digits = 3, big.mark=",", warn = -1)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```


# Introduction

This paper continues our discussion on Seattle housing prices from our first project. As mentioned in our earlier report, "Seattle boasts among 'hottest' housing markets in the country; as of [July 2018](https://bit.ly/2v5UMcn), Seattle 'led the nation in home price gains' for 21 straight months.'" Given this context, our basic SMART question remains the same: how can we predict house prices in Seattle?  This results to be a regression problem on the target variable of price, and we will apply 4 different approaches learned throughout the course to try to solve it.

Included in our discussion is some exploratory data analysis (EDA) from our first assignment, along with some new models, including KNN, ridge and lasso regression, PCA/PCR, and decision trees/random forest. Each model comes with its own advantages and disadvantages, and as such no single model offers total explanatory power. Our hope, however, is that the sum is greater than its parts.

# EDA

The following are excerpts and graphs from the EDA section of our previous report. We are including them here to remind the reader of our dataset's attributes, before we dive into the analysis.

```{r base_lib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
```
```{r loadd, echo=FALSE}
kc_house_data <- read.csv("kc_house_data.csv")
str(kc_house_data)
```
```{r cleand, include=FALSE}
kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

price.ln = log(kc_house_data$price)
```

"A brief overview of the dataset yields the following observations for housing price: the minimum price is \$78,000, while the maximum is \$7,700,000 (quite a large range); the mean of the dataset is \$540,198 (indicating that the dataset is right-skewed, as further indicated by the histogram below); the standard deviation of the dataset is \$367,142; and the variance is 134,792,956,735 (quite large, indicating ["that the data points are very spread out from the mean, and from one another"](https://bit.ly/2MZ1cCn)."

```{r price hist, echo=FALSE}
hist(kc_house_data$price[kc_house_data$price<=2000000], xaxt="n", ylim = c(0,5000), col = heat.colors(20), main = "Housing Price Histogram, $0-$2M Only", xlab = "Housing Price ($)", cex.axis = .75)
axis(side=1, at=axTicks(1), 
     labels=formatC(axTicks(1), format="d", big.mark=','))

```

Below is a visualization of the points in the dataset by price on a map, plotted with the leaflet library.
Note that the data have been divided by unequal bins to provide a better visualization of the distribution of housing price, so please read the legend carefully. More expensive houses tend to be concentrated near the water and center of the city.
 
```{r geo, echo=FALSE}
price.bins <- c(0, 250000, 500000, 750000, 1000000, 1250000, 1500000, 1750000, 2000000, 8000000)
qpal <-  colorBin(palette = 'GnBu', kc_house_data$price, bins= price.bins, n = 9)
house.map1 <- leaflet(kc_house_data) %>% 
  addProviderTiles("CartoDB.Positron") %>% 
  addCircleMarkers(lng = ~long, lat = ~lat, 
                   stroke = FALSE, 
                   fillOpacity = 5, 
                   color = ~qpal(price), 
                   radius = 2,
                   label = ~as.character(paste0('Price: $', price, ", ", 'condition: ', condition, ", ", 'year built: ', yr_built, 'Sqft living: ', sqft_living))) %>%
  addLegend('bottomright', pal = qpal, values = ~price, opacity = 1, title = 'Price', labFormat = labelFormat(prefix = '$', between = ' - $'))

house.map1

```

## Important Features Comparisons

From the scatterplot, it's apparent that there is a relatively strong, positive correlation between housing price and living space (.70192, to be exact). That is, as living space increases, so does housing price. Note that a majority of the data points lie below 6,000 sqft, and below $2 million.

```{r plot sqft living, echo=FALSE}
plot(kc_house_data$sqft_living, kc_house_data$price, xaxt="n", yaxt="n", pch = 20, cex = .1, xlab = "Square Feet of Living Space", ylab = "Housing Price ($)", cex.axis = .75, main = "Housing Price vs. Living Space")
abline(lm(kc_house_data$price ~ kc_house_data$sqft_living, data = kc_house_data), col = "orange")
legend(x='topright', legend=paste('Correlation =',round(cor(kc_house_data$sqft_living, kc_house_data$price),5)))
axis(side=1, at=axTicks(1), 
     labels=formatC(axTicks(1), format="d", big.mark=','))
axis(side=2, at=axTicks(2), 
     labels=formatC(axTicks(2), format="d", big.mark=','))
```

Here we have a boxplot comparing "grade" with housing price. "Grade" represents an index from 1 to 13, with the lowest number representing poor construction and design. The trend is clear: construction and design grade correlate positively with housing price.

```{r, echo=FALSE}
ggplotly(ggplot(kc_house_data, aes(x=grade, y=price, fill=grade)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Housing Price vs. Apartment Grade (log)") + ylab("Housing Price ($)") + xlab("Apartment Grade") +  theme(plot.title= element_text(hjust=0.5, size = 14)) + scale_y_log10(labels = function(x) format(x,nsmall = 2,scientific = FALSE, big.mark = ',')))

```


# KNN

```{r loaddata, include=FALSE}

loadPkg("readr")
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg('class')

kc_house_data <- data.frame(read_csv("kc_house_data.csv"))

kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

```

Here, we convert pass "price" through a log function to better normalize its distribution (unnormalized, it has a heavy right skew). After passing "price" through the log function, we convert "price" to a factor variable and divide it into 3 categories ("Low", "Medium", and "High") to prepare for KNN analysis.

```{r price_categorization, echo=FALSE}

hist(kc_house_data$price)

hist(log(kc_house_data$price))

range(log(kc_house_data$price))

kc_house_data$price <- log(kc_house_data$price)

kc_house_data$price <- cut(kc_house_data$price, breaks = 3, labels = c("Low", "Medium", "High")) # categorization of house prices

summary(kc_house_data$price) # distribution of data points according to their categories

```

Next, we split the data into 80% training, and 20% test subsets.

```{r datasplit, include=FALSE}
set.seed(1)
kc_house_data_train_rows = sample(1:nrow(kc_house_data),     #<- from 1 to the number of 
                                                     #   rows in the data set
                              round(0.8 * nrow(kc_house_data), 0),  #<- multiply the 
                                                                #   number of rows
                                                                #   by 0.8 and round
                                                                #   the decimals
                              replace = FALSE)       #<- don't replace the numbers

# Let's check to make sure we have 80% of the rows. 
length(kc_house_data_train_rows) / nrow(kc_house_data)

kc_house_data_train = kc_house_data[kc_house_data_train_rows, ]  #<- select the rows identified in
                                                     #   the kc_house_data_train_rows data
kc_house_data_test = kc_house_data[-kc_house_data_train_rows, ]  #<- select the rows that weren't 
                                                     #   identified in the
                                                     #   kc_house_data_train_rows data

```

We then apply the chooseK() function to determine the best KNN k value for our dataset. Within the chooseK() function itself, I select features that are truly numeric (KNN requires predictor variables to be numeric). For instance, even though "yr_built" is classified as an "integer" data type, a concept such as year is best thought of as categorical, not numerical. As a result, I chose to exclude this and other similar variables from the analysis.

From the resulting graph, it becomes evident that 12 is approximately the best value for k: it offers the highest accuracy.

```{r chooseK, echo=FALSE}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy. 
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 25, by = 2),  #<- set k to be odd number from 1 to 25
                         function(x) chooseK(x, 
                                             train_set = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
                                             val_set = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
                                             train_class = kc_house_data_train[, "price"],
                                             val_class = kc_house_data_test[, "price"]))

# Reformat the results to graph the results.
# str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

```

Now that we have our k value, we can run our KNN analysis, using the same features we fed the chooseK() function previously.

```{r KNNtraining, echo=FALSE}

# Let's train the classifier for k = 12 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class") 
loadPkg("class")

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
price_predict = knn(train = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],  #<- training set cases
               test = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],    #<- test set cases
               cl = kc_house_data_train[, "price"],                         #<- category for true classification
               k = 12) #,                                                    #<- number of neighbors considered
               # use.all = TRUE)                                            #<- control ties between class assignments
                                                                            #   If true, all distances equal to the kth 
                                                                            #   largest are included

# View the output.
str(price_predict)
#length(price_predict)
table(price_predict)

```

Now let's take a look at the results. Our KNN model classified housing price correctly about 74% of the time, or 3199 out of a possible `r length(price_predict)` test cases.

It misclassified 385 medium priced houses as "low", 661 low priced houses as "medium", 4 medium priced houses as "high", and 70 high priced houses "medium".

KNN is a useful algorithm for classifying data points. We showed that at 74% accuracy, our algorithm successfully predicted housing price categories based on variables such as "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", and "sqft_lot15". If we were in the real estate market and wanted to know generally how high or low we should price a house, we could determine an answer based on these variables.

It should be noted, however, that KNN cannot be used to predict *numeric* prices, since the response variable (price) must be categorical. To predict specific prices, one must use linear regression or PCR.


```{r comparison_of_knnclassification_with_true_class, echo=FALSE}

# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from price_predict to the original data set.
kNN_res = table(price_predict,
                kc_house_data_test$`price`)

kNN_res
#sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)

```

KNN results in an accuracy of `r kNN_acc` eventually. 

# Tree Modeling

We decided to also use a Decision Tree modeling approach for this regression problem, and see if we could also gain interesting visualizations of the dataset to predict the value of price on average.

```{r lib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("rpart") 
loadPkg("caret") 
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
loadPkg("tree") 
loadPkg('ISLR')
loadPkg('randomForest')
loadPkg('leaps')
```

```{r data, include=FALSE}
kc_house_data <- read.csv("kc_house_data.csv")
str(kc_house_data)
```

```{r clean, include=FALSE}
kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

price.ln = log(kc_house_data$price)

```


## Regression Tree

```{r tree, include=FALSE}
tree1 <- tree(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data)
summary(tree1)
```

Using the 'tree' package, we built a tree on all the features used as predictors (i.e. excluding id, date, the various geographic variables and sqft_living15 and sqft_lot15) for price. We used the logarithmic of the price to provide an easier visualization of the tree and allowed more precide average values per node. This tree presented 9 termindal nodes and a mean squared error of 0.12. 

```{r plott, echo=FALSE}
plot(tree1) 
title("Simple Regression Tree")
text(tree1,cex=0.75, digits=3)
```

From the plot of the tree, we can see that the algorithm splits the data using grade, yr_built and sqft_living. As expected as a general trend, if grade is better and sqft_living are larger, price would be higher. While if grade is lower and houses get smaller, price would then be likely to be lower. The splits also show an interesting fact that the age of house correlates positively with its price, as in splits based on yr_built the older the house the higher the average house price would be.
As we used the log of price to predict price with this tree, we can see how the highest average of price in the farthest right leaf is `r exp(14.07)` dollars and the lowest average of price in the farthest left is `r exp(12.52)` dollars.

Furthermore, we use the 'rpart' package to build fancier visualization of decision trees for this dataset.

```{r tree2, include=FALSE}
tree2 <- rpart(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data, cp=.02)
summary(tree2)
par(xpd = NA, mar = rep(0.7, 4)) 
plot(tree2, compress = TRUE)
text(tree2, cex = 0.7, use.n = TRUE, fancy = TRUE, all = TRUE)
```

```{r tree2pl, echo=FALSE}
fancyRpartPlot(tree2, main = "Fancy Regression Tree")

rpart.plot(tree2, box.palette="RdBu", shadow.col="gray", nn=TRUE, main = "Fancier Regression Tree")
```

These trees provide a fairly similar result from the classic tree, but from which we can observe the proportion of sample observations present in each terminal leaf (in this case only 7). Again, the variables used to build the tree are grade, sqft_living and yr_built.  As expected, the right branches of the tree only contain 20% of the houses, as the most expensive houses with better grade than the rest; indeed, the houses that have better grande and sqft superior than 3757, result to be only 5% of the data. Whereas, the largest proportions are within the left branches given their lower grade: 80% of house are of grade between 3 and 8 and 52% of grade 3 to 7. The terminal node resulting with highest proportion is the one containing observations of house of grade 7 and built after 1953, namely 29% of the observations in the dataset.  

On another note, we decided to experiment with trees and build one that would provide an average of prices per different geographic areas. Thus, using the longitude and latitude as predictors for the log of price, we built the following tree which shows the mean value of price per each are on a map, divided by nodes on longitude and latitude. 
```{r maptree, echo=FALSE}
treefit = tree(log(price) ~ long+lat,data=kc_house_data)
price.deciles = quantile(kc_house_data$price,0:10/10)
cut.prices = cut(kc_house_data$price,price.deciles,include.lowest=TRUE)
plot(kc_house_data$long,kc_house_data$lat,col=grey(10:2/11)[cut.prices],pch=20,xlab="Longitude",ylab="Latitude")
title('Regression Tree Map')
partition.tree(treefit,ordvars=c("long","lat"),add=TRUE, col = 'red')
```

This visualization is helpful to note that higher prices are corresponding to the central area, i.e. Seattle downtown and adjacent zones, where the average price is estimated to be `r exp(13.9)`, while areas on the outskirts of the region shows average values of `r exp(12.7)` or `r exp(12.6)`, as one would normally expect.


## Prune tree

Subsequently, we pruned the tree down with a normal 'prune' function. 

```{r prune, echo=FALSE}
tree1.seq <- prune.tree(tree1) # Sequence of pruned tree sizes/errors
plot(tree1.seq)  # error versus plot size
```

First of all, we can observe the plot of a sequence of differenct pruned trees' sizes versus their error rates. The vector of these error rates for the pruning of the tree in our case resulted in `r tree1.seq$dev`.

```{r pruntr, echo=FALSE}
opt.trees = which(tree1.seq$dev == min(tree1.seq$dev)) # Positions of optimal (with respect to error) trees

plot(prune.tree(tree1,best=5))
title('Pruned Regression Tree')
text(prune.tree(tree1,best=5), cex=0.75, digits = 3)
```

As we intended to represent the smallest optimal tree among these pruned trees, this appeared to be of a size equal to `r min(tree1.seq$size[opt.trees])`. The plot then shows a pruned tree with 6 terminal nodes, and splits operated again on grade, sqft_living and yr_built. This tree results simpler to read given the fewer split, although it does not add predictive power while actually taking away some important splits from a tree model that already simplifies the dataset significantly.

## Testing model 

First, to test the model perfomance, we divided the dataset in training and test set, where the latter is simply the first fold of the data. 

```{r splittree, include=FALSE}
fold <- floor(runif(nrow(kc_house_data),1,11)) 
  kc_house_data$fold <- fold
test.set <- kc_house_data[kc_house_data$fold == 1,] 
train.set <- kc_house_data[kc_house_data$fold != 1,] 
```

We then train a regression tree on the basis of the 'rpart' library, using our training dataset this time. The plots below show the cross-validation results of this tree, showing that 8 splits would create an optimal sized tree. This tree presents a root node error of 0.3.

```{r cvtest, echo=FALSE}
tree.pred <- rpart(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set)
#printcp(fit) # display the results
plotcp(tree.pred) # visualize cross-validation results
#summary(fit) # detailed summary of splits

# additional plots
par(mfrow=c(1,2)) # two plots on one page
rsq.rpart(tree.pred) # visualize cross-validation results
```

Building the tree on training dataset using the 'tree' package, we obtain a slighlty different tree pruned to 5 leaves, which splits data on grade first and then sqft_living.

```{r trtest, echo=FALSE}
tree.pred <- tree(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set, mindev=0.001)
treepr <- prune.tree(tree.pred, best = 5) # Return best pruned tree with 5 leaves, evaluating error on training data
plot(treepr) 
title('Training Regression Tree')
text(treepr,cex=0.75, digits=3)

tree.pred.seq <- prune.tree(tree.pred)
plot(tree.pred.seq)
tree.pred.seq$dev
opt.trees = which(tree.pred.seq$dev == min(tree.pred.seq$dev)) 
```

The plot of errors versus size also points out an optimal tree at `r min(tree.pred.seq$size[opt.trees])`  nodes.

```{r predtree, echo=FALSE}
treepr.pred <- prune.tree(tree.pred, best = 5, newdata = test.set) #evaluates on test data
plot(treepr.pred) 
title('Test Regression Tree')
text(treepr.pred,cex=0.75, digits=3)
```

Evaluating this tree on test data, we can see how the trained model did a good job at predicting price for the dataset as errors and tree are almost identical. 


## Random Forest

Ultimately, we used a Random Forest algorithm to evaluate if this type of ensembling could increase the perfomance of the tree model on our dataset.

```{r rf, echo=FALSE}
ff1 <- randomForest(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data = train.set, importance = TRUE)
ff1
```

Thus, we ran a regression random forest on all the predictors variables for the logarithmic of price as target. The model builds 500 trees ensembled with 3 variables tried at each split, and it eventually provides a 68.5% of variance explained and a MSE of 0.0875.


# PCA

Here, we use principle component analysis to predict house price. We chose this method because there are too many variables, and did not know which ones to use. We want to capture as much information as possible by the fewest number of variables. 

```{r libpca, include=FALSE}
library("dplyr")
library("tidyr")
library("tidyverse")
library("ggplot2")
library("Rmisc")
library("MASS")
library("corrplot")
library("faraway")
library("factoextra")
library("pls")
library("magrittr")
```

```{r lod, include=FALSE}
house <- data.frame(read.csv("kc_house_data.csv", header = TRUE))
str(house)
```

## Subset Data 
We deleted some unuseful variables which we presumed were uncorrelated to house price such as lattitude, longitude, the 15 neighborhoods' sqft_living , sqft_lot, the year of renovated, zipcode, date of record, and id. Also, there are several variables containing mostly 0 values, so we also deleted them (these include "view" and "waterfront"). We then took a look at the datset.

```{r subs, echo=FALSE}
house2 <- subset(house,select = -c(id,date,waterfront,view,zipcode,lat,long,sqft_living15,sqft_lot15,yr_renovated))
str(house2)
```
There are 11 variables left and one variable is house, which will be used as an independent variable. All variables are numeric. 

Next, we need to check if there are NA values in the dataset.
```{r nas, include=FALSE}
house2 <- drop_na(house2)
sum(is.na(house2))
```
There are no NA values. 

```{r cor, echo=FALSE}
corx=cor(house2[,-1])
corx
```
Next, we took a look at the correlation of the 10 dependent variables, and we found that they are all related, and some are highly related. This demonstrates that it is difficult to determine which ones are important, so we used PCA to reduce dimensionality. 


## PCA part
As the 10 variables have different scales, it is necessary to scale them before analysis. Performing PCA on un-normalized variables will heavily weight variables with high variances.

We used the prcomp function to perform PCA, and subsequently checked the mean and sd of the variables. Since we scaled the variables, the differences in mean and sd are not large. 
```{r pca, echo=FALSE}
prin_comp <- prcomp(scale(house2[,-1]))
summary(prin_comp)
prin_comp$center ##output the mean of variables
prin_comp$scale ##output the sd of variables
```
We can see that with 7 components, 95% of the variance is explained.

Let's take a look at how the variables form  each component. 
```{r rot, echo=FALSE}
prin_comp$rotation
```
We can see that sqft_above forms up nearly 43% of the first component. The second component consists mostly of sqft_basement, which hovers near 60%. 

We can visualize the variance explained by each component. We can see that the first component explains the most, while the subsequent ones explain less. 
```{r eig, echo=FALSE}
fviz_eig(prin_comp)
```

We can visualize how each component is formed by the different variables. However, the graph is very hard to see – much harder than the rotation vizualization. 
```{r bip, echo=FALSE}
biplot(prin_comp, scale = 0)
##The first principle component looks like sqft_above, the second looks like sqft_basement)
```

```{r bip2, echo=FALSE}
biplot(prin_comp,6:7, scale =0)
```

Visualizing the cumulative proportions of variance, we can see that after 7 components, the curve becomes smooth. 
```{r pve, echo=FALSE}
pr.var <- (prin_comp$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
```


## Predicting using PCA

Finally, we created a PCR model wth the 7 components. Actually, the PCR model is a linear model, so we used the lm function to create it. 
```{r predpca, echo=FALSE}
houseprice <- prin_comp$x
modHouses <- lm(house2$price ~ houseprice[,1:7])
summary(modHouses)
```
We can see that 61.3% of the variance is explained by the independent variables. We do not use accuracy to check whether the model is good or not – the reason being that the difference between the house prices is very large. So using accuracy to check the model would not make sense.

We want to see how good the PCR model is, so we created a full linear model to compare it to. 
```{r fullpca, echo=FALSE}
fullmodel=lm(price~.-sqft_basement,data=house2)
summary(fullmodel)
```
61.8% of the variance is explained by the independent variable in the full linear model. It is a bit better than the PCR model. 

## PCR vs. Full Linear Model: A Comparison 
The R^2 of the full model is 0.618, which is higher than PCR at 0.613. We can also see that both models underestimate the value of house prices over 4e+06.

```{r plotpca, echo=FALSE}
par(mfrow = c(1,2))
plot(house2$price, predict(modHouses), xlab = "actual price", ylab = "Predicted price", main = "PCR", abline(a = 0, b = 1, col = "red"))
plot(house2$price, predict(fullmodel), xlab = "actual price", ylab = "Predicted price", main = "Full model", abline(a = 0, b = 1, col = "red"))
```

## Limitations of PCA 
There are a few price outliers. We did not delete them, however, because we think the high prices are important. 
The response variables must be numeric, even though the grade and condition variables are actually catagorical. PCA also relies on the assumption of normality, but our dataset is heavily right skewed.


# Ridge and Lasso 

```{r baselib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("glmnet")
```

```{r uzscale_fcn, include=FALSE}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
```
```{r data clean, include=FALSE}
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
# kc_house_data$condition <- as.factor(kc_house_data$condition)
# kc_house_data$grade <- as.factor(kc_house_data$grade)
# colnames(kc_house_data)
# nrow(kc_house_data)
```

## The Ridge 
For our dataset, numbers of bedrooms, bathrooms, and floors are categorical variables, so we convert them into factors.
Then we prepare a log scale grid for λ values, from 10^10 to 10^-2 in 100 segments, and then build the ridge model.
Afterwards, we draw a plot of coefficients to see the overall trend. 
```{r splitridge, echo=FALSE}
loadPkg("ISLR")
kc_house_data$bedrooms <- as.factor(kc_house_data$bedrooms)
kc_house_data$bathrooms <- as.factor(kc_house_data$bathrooms)
kc_house_data$floors <- as.factor(kc_house_data$floors)
kc_house_data$condition <- as.factor(kc_house_data$condition)
kc_house_data$grade <- as.factor(kc_house_data$grade)
# str(kc_house_data$condition)
# str(kc_house_data$grade)
# str(kc_house_data$bedrooms)
# str(kc_house_data$bathrooms)
# str(kc_house_data$floors)
house_unscale = uzscale(kc_house_data)
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
plot(ridge.mod)
# colnames(house_unscale)
```

The glmnet( ) function creates 100 models, with our choice of 100 $\lambda$ values. Each model's coefficients are stored in the object we named: ridge.mod  
There are 55 coefficients for each model. The 100 $\lambda$ values are chosen from 0.02 ($10^{-2}$) to $10^{10}$, essentially covering the ordinary least square model ($\lambda$ = 0), and the null/constant model ($\lambda$ approaches infinity).


```{r ridge, include=FALSE}
coef(ridge.mod)[,50]
coef(ridge.mod)[,60] 
```

Because the ridge regression uses the "L2 norm", the coefficients are expected to be smaller when $\lambda$ is large. Our "midpoint" (the 50th percentile) of $\lambda$ equals `r ridge.mod$lambda[50]`, and the sum of squares of coefficients is `r sqrt(sum(coef(ridge.mod)[-1,50]^2))`. Compared to the 60th percentile value (we have a decreasing sequence) $\lambda$ of `r ridge.mod$lambda[60]`, we find the sum of squares of the coefficients to be `r sqrt(sum(coef(ridge.mod)[-1,60]^2))`, about 16 times larger.

The model, however, only has 100 different values of $\lambda$ recorded, so we can use the predict function (part of the R basic stats library) for various different purposes, such as calculating the predicted coefficients for $\lambda$=50, for example.

```{r predridge, echo=FALSE}
predict(ridge.mod,s=50,type="coefficients")[1:55,]
```

### Train and Test sets

Let us split the data into training and test sets, so that we can estimate test errors. The split will be used here for Ridge regression, and later for Lasso regression. 

```{r splitrid, warning=F, echo=FALSE}
loadPkg("dplyr")
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)

x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]

y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
```


```{r predrid, echo=FALSE}
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
```
The test set mean squared error (MSE) is `r mean((ridge.pred-y_test)^2)`. (Keep in mind that we are using standardized scores for $\lambda = 4$.)

```{r mseridge, include=FALSE}
mean((mean(y_train)-y_test)^2) # the test set MSE
```
On the other hand, for the null model ($\lambda$ approaches infinity), the MSE can be found to be `r mean((mean(y_train)-y_test)^2)`. So $\lambda = 4$ reduces the variance by about half, at the expense of bias.

```{r mseridge2, echo=FALSE}
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
```
We could have also used a large $\lambda$ value to find the MSE for the null model. These two methods yield essentially the same answer of `r mean((ridge.pred-y_test)^2)`.

```{r mseridg3, echo=FALSE}
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
#mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:55,]
```

Now for the other extreme special case of small $\lambda$, which is the ordinary least square (OLS) model. We can first use the ridge regression result to predict the $\lambda$ =0 case. The MSE was found to be `r mean((ridge.pred - y_test)^2)` using this result. 

We can also build the OLS model directly.
```{r, echo=FALSE}
ols.mod = lm(price~., data = train)
summary(ols.mod)
```
The MSE for OLS regression is `r mean(residuals(ols.mod)^2)`

### Use Cross-validation

We use a built-in cross-validation method with glmnet, which will select the minimal $\lambda$ value.

```{r cvridge, echo=FALSE}
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out)
bestlam = cv.out$lambda.min
```

The minimal $\lambda$ value minimizing training MSE results to be `r cv.out$lambda.min` in this case.

```{r cvridpred, echo=FALSE}
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
#mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:64,]
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((ridge.pred - y_test)^2)
rsq <- 1 - sse / sst
```
The first vertical dotted line shows that the lowest MSE is `r mean((ridge.pred-y_test)^2)`. The second vertical dotted line is within one standard error. Then we calculate the R squared value. R squared is `r rsq` for the ridge model.

## The Lasso

The same function, glmnet( ), with alpha set to 1 will build the Lasso regression model. 
Then we draw the plot for different $\lambda$ values to see the overall trend.  
```{r lasso, echo=FALSE}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
```

```{r lassocoef, echo=FALSE} 
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
#mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:64,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
```
Here, we see that the lowest MSE is when $\lambda$ equals `r cv.out$lambda.min`. It has about 47 non-zero coefficients. The MSE is then `r mean((lasso.pred-y_test)^2)`.

```{r lasso R2, echo=FALSE}
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
```
We then calculate the R squared of lasso regression, which is `r rsq1`.  

Lasso regression is also a good tool for feature selection. So we build a linear model by using lasso to select variables.
```{r lmlasso, echo=FALSE}
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
```

The condition p-values are all lower than 0.05. We consequently remove them from the model and rebuild it.

```{r lmlasso1, echo=FALSE}
lasLin1 <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin1)
vif(lasLin)
```
The R squared value is `r summary(lasLin1)$adj.r.squared`, which is better than what was achieved with ridge and lasso regression.


# Conclusion

In the end, each of our models comes with its advantages and disadvantages. Although KNN proved 74% accurate at classifying prices into "low", "medium" and "high" categories, these categories ultimately do not tell us much, considering their large ranges. Decision trees provide simple visualizations and tell us which features are the most import, but it oversimplies the dataset and yields a low amount of variance explained (even with the random forest included). PCA and PCR yield relatively low amounts of variance explained (around 60%), and they do not differ much from the variance explained by a full linear model. The ridge and lasso regressions also do not perform as well as the full linear model (they have lower R2 values). **To summarize our findings, in general, linear regression tends to offer the most explanatory power, and "sqft_living" and "grade" seem to influence price the most.** This makes intuitive sense: living space and quality of construction are the most important variables when it comes to housing price, and the simple linear nature of this regression problem accepts this simple predictive model as quite efficient and strong indeed.


# Bibliography

Dataset available: https://www.kaggle.com/harlfoxem/housesalesprediction/data

Perry, M. J. (2016, June 5). New US homes today are 1,000 square feet larger than in 1973 and living space per person has nearly doubled. Retrieved from https://www.aei.org/carpe-diem/new-us-homes-today-are-1000-square-feet-larger-than-in-1973-and-living-space-per-person-has-nearly-doubled/

Roberts, D. (n.d.). Variance and Standard Deviation. Retrieved from https://mathbitsnotebook.com/Algebra1/StatisticsData/STSD.html

Rosenberg, M. (2018, July 31). Seattle-area home prices this spring rose at fastest rate since 2006 bubble. The Seattle Times. Retrieved from https://www.seattletimes.com/business/real-estate/seattle-area-home-prices-this-spring-rose-at-fastest-rate-since-2006-bubble/



