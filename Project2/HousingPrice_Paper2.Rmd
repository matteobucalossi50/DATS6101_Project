---
title: "Housing Market - Code"
author: "Anti-Code Group"
date: "11/26/2019"
output:  
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

https://www.kaggle.com/harlfoxem/housesalesprediction/data

https://github.com/matteobucalossi50/DATS6101_Project

# Introduction

This paper continues our discussion on Seattle housing prices from our first project. As mentioned in our earlier report, "Seattle boasts among 'hottest' housing markets in the country; as of [July 2018](https://bit.ly/2v5UMcn), Seattle 'led the nation in home price gains' for 21 straight months.'" Given this context, our basic SMART question remains the same: how can we predict house prices in Seattle?

Included in our discussion is some exploratory data analysis (EDA) from our first assignment, along with some new models, including KNN, ridge and lasso regression, random forest, PCA, PCR, and decision trees. Each model comes with its own advantages and disadvantages, and as such no single model offers total explanatory power. Our hope, however, is that the sum is greater than its parts.

# EDA

The following are excerpts and graphs from the EDA section of our previous report. We are including them here to remind the reader of our dataset's attributes, before we dive into the analysis.

"A brief overview of the dataset yields the following observations for housing price: the minimum price is \$78,000, while the maximum is \$7,700,000 (quite a large range); the mean of the dataset is \$540,198 (indicating that the dataset is right-skewed, as further indicated by the histogram below); the standard deviation of the dataset is \$367,142; and the variance is 134,792,956,735 (quite large, indicating ["that the data points are very spread out from the mean, and from one another"](https://bit.ly/2MZ1cCn)."

```{r price hist, echo=FALSE}
hist(kc_house_data$price[kc_house_data$price<=2000000], xaxt="n", ylim = c(0,5000), col = heat.colors(20), main = "Housing Price Histogram, $0-$2M Only", xlab = "Housing Price ($)", cex.axis = .75)
axis(side=1, at=axTicks(1), 
     labels=formatC(axTicks(1), format="d", big.mark=','))

```

### Comparison of sqft living with price

From the scatterplot, it's apparent that there is a relatively strong, positive correlation between housing price and living space (.70192, to be exact). That is, as living space increases, so does housing price. Note that a majority of the data points lie below 6,000 sqft, and below $2 million.

```{r plot sqft living, echo=FALSE}
plot(kc_house_data$sqft_living, kc_house_data$price, xaxt="n", yaxt="n", pch = 20, cex = .1, xlab = "Square Feet of Living Space", ylab = "Housing Price ($)", cex.axis = .75, main = "Housing Price vs. Living Space")
abline(lm(kc_house_data$price ~ kc_house_data$sqft_living, data = kc_house_data), col = "orange")
legend(x='topright', legend=paste('Correlation =',round(cor(kc_house_data$sqft_living, kc_house_data$price),5)))
axis(side=1, at=axTicks(1), 
     labels=formatC(axTicks(1), format="d", big.mark=','))
axis(side=2, at=axTicks(2), 
     labels=formatC(axTicks(2), format="d", big.mark=','))
```

Here we have a boxplot comparing "grade" with housing price. "Grade" represents an index from 1 to 13, with the lowest number representing poor construction and design. The trend is clear: construction and design grade correlate positively with housing price.

```{r, echo=FALSE}
ggplotly(ggplot(kc_house_data, aes(x=grade, y=price, fill=grade)) + geom_boxplot() + scale_fill_brewer(palette="Spectral") + ggtitle("Housing Price vs. Apartment Grade (log)") + ylab("Housing Price ($)") + xlab("Apartment Grade") +  theme(plot.title= element_text(hjust=0.5, size = 14)) + scale_y_log10(labels = function(x) format(x,nsmall = 2,scientific = FALSE, big.mark = ',')))

```

# KNN

```{r loaddata, include=FALSE}

loadPkg("readr")
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg('class')

kc_house_data <- data.frame(read_csv("~/Downloads/kc_house_data.csv"))

kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

```

Here, we convert pass "price" through a log function to better normalize its distribution (unnormalized, it has a heavy right skew). After passing "price" through the log function, we convert "price" to a factor variable and divide it into 3 categories ("Low", "Medium", and "High") to prepare for KNN analysis.

```{r price_categorization, echo=FALSE}

hist(kc_house_data$price)

hist(log(kc_house_data$price))

range(log(kc_house_data$price))

kc_house_data$price <- log(kc_house_data$price)

kc_house_data$price <- cut(kc_house_data$price, breaks = 3, labels = c("Low", "Medium", "High")) # categorization of house prices

summary(kc_house_data$price) # distribution of data points according to their categories

```

Next, we split the data into 80% training, and 20% test subsets.

```{r datasplit, echo=FALSE}
set.seed(1)
kc_house_data_train_rows = sample(1:nrow(kc_house_data),     #<- from 1 to the number of 
                                                     #   rows in the data set
                              round(0.8 * nrow(kc_house_data), 0),  #<- multiply the 
                                                                #   number of rows
                                                                #   by 0.8 and round
                                                                #   the decimals
                              replace = FALSE)       #<- don't replace the numbers

# Let's check to make sure we have 80% of the rows. 
length(kc_house_data_train_rows) / nrow(kc_house_data)

kc_house_data_train = kc_house_data[kc_house_data_train_rows, ]  #<- select the rows identified in
                                                     #   the kc_house_data_train_rows data
kc_house_data_test = kc_house_data[-kc_house_data_train_rows, ]  #<- select the rows that weren't 
                                                     #   identified in the
                                                     #   kc_house_data_train_rows data

```

We then apply the chooseK() function to determine the best KNN k value for our dataset. Within the chooseK() function itself, I select features that are truly numeric (KNN requires predictor variables to be numeric). For instance, even though "yr_built" is classified as an "integer" data type, a concept such as year is best thought of as categorical, not numerical. As a result, I chose to exclude this and other similar variables from the analysis.

From the resulting graph, it becomes evident that 12 is approximately the best value for k: it offers the highest accuracy.

```{r chooseK, echo=FALSE}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy. 
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 25, by = 2),  #<- set k to be odd number from 1 to 25
                         function(x) chooseK(x, 
                                             train_set = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
                                             val_set = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
                                             train_class = kc_house_data_train[, "price"],
                                             val_class = kc_house_data_test[, "price"]))

# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

```

Now that we have our k value, we can run our KNN analysis, using the same features we fed the chooseK() function previously.

```{r KNNtraining, echo=FALSE}

# Let's train the classifier for k = 12 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class") 
loadPkg("class")

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
price_predict = knn(train = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],  #<- training set cases
               test = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],    #<- test set cases
               cl = kc_house_data_train[, "price"],                         #<- category for true classification
               k = 12) #,                                                    #<- number of neighbors considered
               # use.all = TRUE)                                            #<- control ties between class assignments
                                                                            #   If true, all distances equal to the kth 
                                                                            #   largest are included

# View the output.
str(price_predict)
length(price_predict)
table(price_predict)

```

Now let's take a look at the results. Our KNN model classified housing price correctly about 74% of the time, or 3199 out of a possible 4319 test cases.

It misclassified 385 medium priced houses as "low", 661 low priced houses as "medium", 4 medium priced houses as "high", and 70 high priced houses "medium".

KNN is a useful algorithm for classifying data points. We showed that at 74% accuracy, our algorithm successfully predicted housing price categories based on variables such as "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", and "sqft_lot15". If we were in the real estate market and wanted to know generally how high or low we should price a house, we could determine an answer based on these variables.

It should be noted, however, that KNN cannot be used to predict *numeric* prices, since the response variable (price) must be categorical. To predict specific prices, one must use linear regression or PCR.


```{r comparison_of_knnclassification_with_true_class, echo=FALSE}

# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from price_predict to the original data set.
kNN_res = table(price_predict,
                kc_house_data_test$`price`)

kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc


```

# Trees

```{r lib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("rpart") 
loadPkg("caret") 
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
loadPkg("tree") 
loadPkg('ISLR')
loadPkg('randomForest')
loadPkg('leaps')
```

## Loading and Cleaning the Data

```{r data, echo=FALSE}
kc_house_data <- read.csv("kc_house_data.csv")
str(kc_house_data)
```

```{r clean, include=FALSE}
kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

price.ln = log(kc_house_data$price)

```


# Regression Tree

```{r tree, echo=FALSE}
tree1 <- tree(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data)
summary(tree1)
```

The tree built on all the features used as predictors (excluding id, date, geographic variables and sqft_living15 and sqft_lot15) for price, and presents 9 terminal nodes and a mean squared error of 0.12.

```{r plott, echo=FALSE}
plot(tree1) 
text(tree1,cex=0.75, digits=3)
```

We can see that the algorithm splits the data using grade, yr_built and sqft_living. As expected, if grade is better and sqft_living are larger, price is higher. While if grade is lower and houses get older and smaller, price trends lower.
As we used the log of price to predict price with this tree, we can see how the highest average of price in the farthest right leaf is `r exp(14.07)` dollars and the lowest average of price in the farthest left is `r exp(12.52)` dollars.

```{r tree2, echo=FALSE}
tree2 <- rpart(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data, cp=.02)

par(xpd = NA, mar = rep(0.7, 4)) 
plot(tree2, compress = TRUE)
text(tree2, cex = 0.7, use.n = TRUE, fancy = TRUE, all = TRUE)
fancyRpartPlot(tree2)

rpart.plot(tree2, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```

Using the package rpart, we can build a tree that provides a similar result but from which we can observe the proportion of sample observations present in each terminal leaf (in this case 7). As expected, the right branches of the tree only contain 20% of the houses, while the largest proportion are on the left branches given their lower price.

```{r maptree, echo=FALSE}
treefit = tree(log(price) ~ long+lat,data=kc_house_data)
plot(treefit)
text(treefit, cex=0.75, digits = 3)
price.deciles = quantile(kc_house_data$price,0:10/10)
cut.prices = cut(kc_house_data$price,price.deciles,include.lowest=TRUE)
plot(kc_house_data$long,kc_house_data$lat,col=grey(10:2/11)[cut.prices],pch=20,xlab="Longitude",ylab="Latitude")
partition.tree(treefit,ordvars=c("long","lat"),add=TRUE, col = 'red')
```

## Prune tree

```{r prune, echo=FALSE}
tree1.seq <- prune.tree(tree1) # Sequence of pruned tree sizes/errors
plot(tree1.seq)  # error versus plot size
tree1.seq$dev # Vector of error rates for prunings in order 

opt.trees = which(tree1.seq$dev == min(tree1.seq$dev)) # Positions of optimal (with respect to error) trees
min(tree1.seq$size[opt.trees]) # Size of smallest optimal tree

prune.tree(tree1,best=5) # Returns best pruned tree with all data
plot(prune.tree(tree1,best=5))
text(prune.tree(tree1,best=5), cex=0.75, digits = 3)
```
Plotting the error versus the size of trees, we can see that the optimal pruned tree (smallest tree minimizing the errors) would have 9 nodes.

## Testing the model 

```{r splittree, echo=FALSE}
fold <- floor(runif(nrow(kc_house_data),1,11)) 
  kc_house_data$fold <- fold
test.set <- kc_house_data[kc_house_data$fold == 1,] 
train.set <- kc_house_data[kc_house_data$fold != 1,] 
```

```{r test, echo=FALSE}
tree.pred <- tree(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set, mindev=0.001)
treepr <- prune.tree(tree.pred, best = 5) # Return best pruned tree with 5 leaves, evaluating error on training data
treepr
plot(treepr) 
text(treepr,cex=0.75, digits=3)

tree.pred.seq <- prune.tree(tree.pred)
plot(tree.pred.seq)
tree.pred.seq$dev
opt.trees = which(tree.pred.seq$dev == min(tree.pred.seq$dev)) 
min(tree.pred.seq$size[opt.trees]) 
```

Building the trees on the training dataset, we obtain a slightly different tree pruned to 5 leaves, which splits data on grade first and then sqft_living. The plot of errors versus size also points out an optimal tree at 34 nodes.

```{r predtree, echo=FALSE}
treepr.pred <- prune.tree(tree.pred, best = 5, newdata = test.set) #evaluates on test data
treepr.pred
plot(treepr.pred) 
text(treepr.pred,cex=0.75, digits=3)
```

Evaluating this tree on test data, we can see how the trained model did a good job of predicting price for the dataset, as the errors and tree are almost identical.

# Random Forest

```{r rf, echo=FALSE}
ff1 <- randomForest(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data = train.set, importance = TRUE)
ff1
```

A regression random forest on all the predictor variables for price leads to 500 trees for 3 variables tried at each split, and eventually provides 68.5% of the variance explained, and an MSE of 0.0875.


# PCA
Here, we use principle component analysis to predict house price. We chose this method because there are too many variables, and did not know which ones to use. We want to capture as much information as possible by the fewest number of variables. 

```{r libpca, include=FALSE}
library("dplyr")
library("tidyr")
library("tidyverse")
library("ggplot2")
library("Rmisc")
library("MASS")
library("corrplot")
library("faraway")
library("factoextra")
library("pls")
library("magrittr")
```

First, we loaded the dataset into Rstudio, renamed it "house" and took a look at the structure of the dataset.

```{r lod, echo=FALSE}
house <- data.frame(read.csv("kc_house_data.csv", header = TRUE))
str(house)
```
There are 21613 oberservations on 21 variables.

## Subset Data 
We deleted some unuseful variables which we presumed were uncorrelated to house price such as lattitude, longitude, the 15 neighborhoods' sqft_living , sqft_lot, the year of renovated, zipcode, date of record, and id. Also, there are several variables containing mostly 0 values, so we also deleted them (these include "view" and "waterfront"). We then took a look at the datset.

```{r subs, echo=FALSE}
house2 <- subset(house,select = -c(id,date,waterfront,view,zipcode,lat,long,sqft_living15,sqft_lot15,yr_renovated))
str(house2)
```
There are 11 variables left and one variable is house, which will be used as an independent variable. All variables are numeric. 

Next, we need to check if there are NA values in the dataset.
```{r nas, include=TRUE}
house2 <- drop_na(house2)
sum(is.na(house2))
```
There are no NA values. 


```{r cor, echo=FALSE}
corx=cor(house2[,-1])
corx
```
Next, we took a look at the correlation of the 10 dependent variables, and we found that they are all related, and some are highly related. This demonstrates that it is difficult to determine which ones are important, so we used PCA to reduce dimensionality. 



## PCA part
As the 10 variables have different scales, it is necessary to scale them before analysis. Performing PCA on un-normalized variables will heavily weight variables with high variances.

We used the prcomp function to perform PCA, and subsequently checked the mean and sd of the variables. Since we scaled the variables, the differences in mean and sd are not large. 
```{r pca, echo=FALSE}
prin_comp <- prcomp(scale(house2[,-1]))
summary(prin_comp)
prin_comp$center ##output the mean of variables
prin_comp$scale ##output the sd of variables
```
We can see that with 7 components, 95% of the variance is explained.

Let's take a look at how the variables form  each component. 
```{r rot, echo=FALSE}
prin_comp$rotation
```
We can see that sqft_above forms up nearly 43% of the first component. The second component consists mostly of sqft_basement, which hovers near 60%. 

We can visualize the variance explained by each component. We can see that the first component explains the most, while the subsequent ones explain less. 
```{r eig, echo=FALSE}
fviz_eig(prin_comp)
```

We can visualize how each component is formed by the different variables. However, the graph is very hard to see – much harder than the rotation vizualization. 
```{r bip, echo=FALSE}
biplot(prin_comp, scale = 0)
##The first principle component looks like sqft_above, the second looks like sqft_basement)
```

```{r bip2, echo=FALSE}
biplot(prin_comp,6:7, scale =0)
```

Visualizing the cumulative proportions of variance, we can see that after 7 components, the curve becomes smooth. 
```{r pve, echo=FALSE}
pr.var <- (prin_comp$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
```


## Predicting using PCA
Finally, we created a PCR model wth the 7 components. Actually, the PCR model is a linear model, so we used the lm function to create it. 
```{r predpca, echo=FALSE}
houseprice <- prin_comp$x
modHouses <- lm(house2$price ~ houseprice[,1:7])
summary(modHouses)
```
We can see that 61.3% of the variance is explained by the independent variables. We do not use accuracy to check whether the model is good or not – the reason being that the difference between the house prices is very large. So using accuracy to check the model would not make sense.

We want to see how good the PCR model is, so we created a full linear model to compare it to. 
```{r fullpca, echo=FALSE}
fullmodel=lm(price~.-sqft_basement,data=house2)
summary(fullmodel)
```
61.8% of the variance is explained by the independent variable in the full linear model. It is a bit better than the PCR model. 

## PCR vs. Full Linear Model: A Comparison 

The R^2 of the full model is 0.618, which is higher than PCR at 0.613. We can also see that both models underestimate the value of house prices over 4e+06.

```{r plotpca, echo=FALSE}
par(mfrow = c(1,2))
plot(house2$price, predict(modHouses), xlab = "actual price", ylab = "Predicted price", main = "PCR", abline(a = 0, b = 1, col = "red"))
plot(house2$price, predict(fullmodel), xlab = "actual price", ylab = "Predicted price", main = "Full model", abline(a = 0, b = 1, col = "red"))
```

## Limitations of PCA 
There are a few price outliers. We did not delete them, however, because we think the high prices are important. 
The response variables must be numeric, even though the grade and condition variables are actually catagorical. PCA also relies on the assumption of normality, but our dataset is heavily right skewed.

# Ridge and Lasso 

```{r baselib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("glmnet")
```

```{r uzscale_fcn, include=FALSE}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
```

We first load the dataset and remove columns that would be used in the regression. Then we drop the NA values from the dataset.
```{r data clean, include=FALSE}
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
# kc_house_data$condition <- as.factor(kc_house_data$condition)
# kc_house_data$grade <- as.factor(kc_house_data$grade)
# colnames(kc_house_data)
# nrow(kc_house_data)
```

### The Ridge 
For our dataset, numbers of bedrooms, bathrooms, and floors are categorical variables, so we convert them into factors.
Then we prepare a log scale grid for λ values, from 10^10 to 10^-2 in 100 segments, and then build the ridge model.
Afterwards, we draw a plot of coefficients to see the overall trend. 
```{r splitridge, echo=FALSE}
loadPkg("ISLR")
kc_house_data$bedrooms <- as.factor(kc_house_data$bedrooms)
kc_house_data$bathrooms <- as.factor(kc_house_data$bathrooms)
kc_house_data$floors <- as.factor(kc_house_data$floors)
kc_house_data$condition <- as.factor(kc_house_data$condition)
kc_house_data$grade <- as.factor(kc_house_data$grade)
# str(kc_house_data$condition)
# str(kc_house_data$grade)
# str(kc_house_data$bedrooms)
# str(kc_house_data$bathrooms)
# str(kc_house_data$floors)
house_unscale = uzscale(kc_house_data)
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
plot(ridge.mod)
# colnames(house_unscale)
```

The glmnet( ) function creates 100 models, with our choice of 100 $\lambda$ values. Each model's coefficients are stored in the object we named: ridge.mod  
There are 55 coefficients for each model. The 100 $\lambda$ values are chosen from 0.02 ($10^{-2}$) to $10^{10}$, essentially covering the ordinary least square model ($\lambda$ = 0), and the null/constant model ($\lambda$ approaches infinity).


```{r ridge, echo=FALSE}
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60] 
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
```

Because the ridge regression uses the "L2 norm", the coefficients are expected to be smaller when $\lambda$ is large. Our "midpoint" (the 50th percentile) of $\lambda$ equals 11,498, and the sum of squares of coefficients is 0.000155. Compared to the 60th percentile value (we have a decreasing sequence) $\lambda$ of 705, we find the sum of squares of the coefficients to be 0.0025, about 16 times larger.

The model, however, only has 100 different values of $\lambda$ recorded, so we can use the predict function (part of the R basic stats library) for various different purposes, such as calculating the predicted coefficients for $\lambda$=50, for example.

```{r predridge, echo=FALSE}
predict(ridge.mod,s=50,type="coefficients")[1:55,]
```

### Train and Test sets

Let us split the data into training and test sets, so that we can estimate test errors. The split will be used here for Ridge regression, and later for Lasso regression. 

```{r splitrid, warning=F, echo=FALSE}
loadPkg("dplyr")
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)

x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]

y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
```


```{r predrid, echo=FALSE}
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
mean((ridge.pred-y_test)^2)
```
The test set mean squared error (MSE) is `r mean((ridge.pred-y_test)^2)`. (Keep in mind that we are using standardized scores for $\lambda = 4$.)

```{r mseridge, echo=FALSE}
mean((mean(y_train)-y_test)^2) # the test set MSE
```
On the other hand, for the null model ($\lambda$ approaches infinity), the MSE can be found to be `r mean((mean(y_train)-y_test)^2)`. So $\lambda = 4$ reduces the variance by about half, at the expense of bias.

```{r mseridge2, echo=FALSE}
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((ridge.pred-y_test)^2)
```
We could have also used a large $\lambda$ value to find the MSE for the null model. These two methods yield essentially the same answer of `r mean((ridge.pred-y_test)^2)`.

```{r mseridg3, echo=FALSE}
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:55,]
```

Now for the other extreme special case of small $\lambda$, which is the ordinary least square (OLS) model. We can first use the ridge regression result to predict the $\lambda$ =0 case. The MSE was found to be `r mean((ridge.pred - y_test)^2)` using this result. 

We can also build the OLS model directly.
```{r, echo=FALSE}
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2)
```
The MSE for OLS regression is `r mean(residuals(ols.mod)^2)`

### Use Cross-validation

We use a built-in cross-validation method with glmnet, which will select the minimal $\lambda$ value.

```{r cvridge, echo=FALSE}
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out)
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
```

```{r cvridpred, echo=FALSE}
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:64,]
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((ridge.pred - y_test)^2)
rsq <- 1 - sse / sst
rsq
```
The first vertical dotted line shows that the lowest MSE is `r mean((ridge.pred-y_test)^2)`. The second vertical dotted line is within one standard error. Then we calculate the R squared value. R squared is `r rsq` for the ridge model.

### The Lasso

The same function, glmnet( ), with alpha set to 1 will build the Lasso regression model. 
Then we draw the plot for different $\lambda$ values to see the overall trend.  
```{r lasso, echo=FALSE}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
```
Here, we see that the lowest MSE is when $\lambda$ equals 0.369. It has about 47 non-zero coefficients. 

```{r lassocoef, echo=FALSE} 
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:64,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
```

```{r lasso R2, echo=FALSE}
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
rsq1
```
We then calculate the R squared of lasso regression, which is `r rsq1`.  

Lasso regression is also a good tool for feature selection. So we build a linear model by using lasso to select variables.
```{r lmlasso, echo=FALSE}
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
```

The condition p-values are all lower than 0.05. We consequently remove them from the model and rebuild it.

```{r lmlasso1, echo=FALSE}
lasLin1 <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin1)
vif(lasLin)
```
The R squared value is `r summary(lasLin1)$adj.r.squared`, which is better than what was achieved with ridge and lasso regression.

# Conclusion

In the end, each of our models comes with its advantages and disadvantages. Although KNN proved 74% accurate at classifying prices into "low", "medium" and "high" categories, these categories ultimately do not tell us much, considering their large ranges. Decision trees provide simple visualizations and tell us which features are the most import, but it oversimplies the dataset and yields a low amount of variance explained (even with the random forest included). PCA and PCR yield relatively low amounts of variance explained (around 60%), and they do not differ much from the variance explained by a full linear model. The ridge and lasso regressions also do not perform as well as the full linear model (they have lower R2 values). **To summarize our findings, in general, linear regression tends to offer the most explanatory power, and "sqft_living" and "grade" seem to influence price the most.** This makes intuitive sense: living space and quality of construction are the most important variables when it comes to housing price.



