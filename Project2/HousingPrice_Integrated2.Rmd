---
title: "Housing Market - Code"
author: "Anti-Code Group"
date: "11/26/2019"
output:  
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

# KNN

```{r loaddata, include=FALSE}
loadPkg("readr")
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg('class')
kc_house_data <- data.frame(read_csv("kc_house_data.csv"))
kc_house_data <- subset(kc_house_data, select = -c(9, 10))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
kc_house_data$condition <- as.factor(kc_house_data$condition)
kc_house_data$grade <- as.factor(kc_house_data$grade)
```

Here, we convert "price" to a factor variable and divide it into 3 categories ("Low", "Medium", and "High") to prepare for KNN analysis.

```{r price_categorization, echo=TRUE}
kc_house_data$price <- cut(kc_house_data$price, breaks = 3, labels = c("Low", "Medium", "High")) # categorization of house prices
summary(kc_house_data$price) # distribution of data points according to their categories
```

Next, we split the data into 80% training, and 20% test subsets.

```{r datasplit, echo=TRUE}
set.seed(1)
kc_house_data_train_rows = sample(1:nrow(kc_house_data),     #<- from 1 to the number of 
                                                     #   rows in the data set
                              round(0.8 * nrow(kc_house_data), 0),  #<- multiply the 
                                                                #   number of rows
                                                                #   by 0.8 and round
                                                                #   the decimals
                              replace = FALSE)       #<- don't replace the numbers
# Let's check to make sure we have 80% of the rows. 
length(kc_house_data_train_rows) / nrow(kc_house_data)
kc_house_data_train = kc_house_data[kc_house_data_train_rows, ]  #<- select the rows identified in
                                                     #   the kc_house_data_train_rows data
kc_house_data_test = kc_house_data[-kc_house_data_train_rows, ]  #<- select the rows that weren't 
                                                     #   identified in the
                                                     #   kc_house_data_train_rows data
```

We then apply the chooseK() function to determine the best KNN k value for our dataset. Within the chooseK() function itself, I select features that are truly numeric (KNN requires predictor variables to be numeric). For instance, even though "yr_built" is classified as an "integer" data type, a concept such as year is best thought of as categorical, not numerical. As a result, I chose to exclude this and other similar variables from the analysis.

From the resulting graph, it becomes evident that 5 is approximately the best value for k: it offers the highest accuracy before the change in accuracy begins to trail off.

```{r chooseK, echo=FALSE}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy. 
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}
# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 25, by = 2),  #<- set k to be odd number from 1 to 25
                         function(x) chooseK(x, 
                                             train_set = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
                                             val_set = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],
                                             train_class = kc_house_data_train[, "price"],
                                             val_class = kc_house_data_test[, "price"]))
# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])
# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")
ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
```

Now that we have our k value, we can run our KNN analysis, using the same features we fed the chooseK() function previously.

```{r KNNtraining, echo=TRUE}
# Let's train the classifier for k = 5 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class") 
loadPkg("class")
# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
price_predict = knn(train = kc_house_data_train[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],  #<- training set cases
               test = kc_house_data_test[, c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15")],    #<- test set cases
               cl = kc_house_data_train[, "price"],                         #<- category for true classification
               k = 5) #,                                                    #<- number of neighbors considered
               # use.all = TRUE)                                            #<- control ties between class assignments
                                                                            #   If true, all distances equal to the kth 
                                                                            #   largest are included
# View the output.
str(price_predict)
length(price_predict)
table(price_predict)
```

Now let's take a look at the results. Our KNN model classified housing price correctly about 99.6% of the time, or 4305 out of a possible 4319 test cases. That's quite good.

It misclassified 14 medium priced houses as "low". It's understandable that the algorithm would misclassify medium priced houses as "low", since those categories are next to each other.

KNN is a useful algorithm for classifying data points. We showed that at 99.6% accuracy, our algorithm successfully predicted housing price categories based on variables such as "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "sqft_above", "sqft_basement", "sqft_living15", and "sqft_lot15". If we were in the real estate market and wanted to know how high or low we should price a house, we could determine an answer based on these variables.


```{r comparison_of_knnclassification_with_true_class, echo=TRUE}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from price_predict to the original data set.
kNN_res = table(price_predict,
                kc_house_data_test$`price`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples
# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]
# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```

# Trees

```{r lib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("rpart") 
loadPkg("caret") 
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
loadPkg("tree") 
loadPkg('ISLR')
loadPkg('randomForest')
loadPkg('leaps')
```

## Loading Data and Clean

https://www.kaggle.com/harlfoxem/housesalesprediction/data

https://github.com/matteobucalossi50/DATS6101_Project

```{r data, echo=FALSE}
kc_house_data <- read.csv("kc_house_data.csv")
str(kc_house_data)
```

```{r clean, include=FALSE}
kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

price.ln = log(kc_house_data$price)

```


## Regression Tree

```{r tree, echo=FALSE}
tree1 <- tree(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data)
summary(tree1)
```

The tree built on all the features used as predictors (namely excluding id, date, geographic variables and sqft_living15 and sqft_lot15) for price, presents 9 termindal nodes and a mean squared error of 0.12.

```{r plott, echo=FALSE}
plot(tree1) 
text(tree1,cex=0.75, digits=3)
```

We can see that the algorithm splits the data using grade, yr_built and sqft_living. As expected, if grade is better and sqft_living are larger, price would be higher. While if grade is lower and houses get older and smaller, price would then be likely lower.
As we used the log of price to predict price with this tree, we can see how the highest average of price in the farthest right leaf is `r exp(14.07)` dollars and the lowest average of price in the farthest left is `r exp(12.52)` dollars.

```{r tree2, echo=FALSE}
tree2 <- rpart(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data, cp=.02)

par(xpd = NA, mar = rep(0.7, 4)) 
plot(tree2, compress = TRUE)
text(tree2, cex = 0.7, use.n = TRUE, fancy = TRUE, all = TRUE)
fancyRpartPlot(tree2)

rpart.plot(tree2, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```

Using the package rpart, we can build a tree that provides a similar result but from which we can observe the proportion of sample observations present in each terminal leaf (in this case 7). As expected, the right branches of the tree only contain 20% of the houses, while the largest proportion are on the left branches given their lower price.

```{r maptree, echo=FALSE}
treefit = tree(log(price) ~ long+lat,data=kc_house_data)
plot(treefit)
text(treefit, cex=0.75, digits = 3)
price.deciles = quantile(kc_house_data$price,0:10/10)
cut.prices = cut(kc_house_data$price,price.deciles,include.lowest=TRUE)
plot(kc_house_data$long,kc_house_data$lat,col=grey(10:2/11)[cut.prices],pch=20,xlab="Longitude",ylab="Latitude")
partition.tree(treefit,ordvars=c("long","lat"),add=TRUE, col = 'red')
```

## Prune tree

```{r prune, echo=FALSE}
tree1.seq <- prune.tree(tree1) # Sequence of pruned tree sizes/errors
plot(tree1.seq)  # error versus plot size
tree1.seq$dev # Vector of error rates for prunings in order 

opt.trees = which(tree1.seq$dev == min(tree1.seq$dev)) # Positions of optimal (with respect to error) trees
min(tree1.seq$size[opt.trees]) # Size of smallest optimal tree

prune.tree(tree1,best=5) # Returns best pruned tree with all data
plot(prune.tree(tree1,best=5))
text(prune.tree(tree1,best=5), cex=0.75, digits = 3)
```
Plotting the error versus the size of trees, we can see that the optimal pruned tree (smallest tree minimizing the errors) would have 9 nodes.

## Testing model 

```{r splittree, echo=FALSE}
fold <- floor(runif(nrow(kc_house_data),1,11)) 
  kc_house_data$fold <- fold
test.set <- kc_house_data[kc_house_data$fold == 1,] 
train.set <- kc_house_data[kc_house_data$fold != 1,] 
```

```{r test, echo=FALSE}
tree.pred <- tree(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set, mindev=0.001)
treepr <- prune.tree(tree.pred, best = 5) # Return best pruned tree with 5 leaves, evaluating error on training data
treepr
plot(treepr) 
text(treepr,cex=0.75, digits=3)

tree.pred.seq <- prune.tree(tree.pred)
plot(tree.pred.seq)
tree.pred.seq$dev
opt.trees = which(tree.pred.seq$dev == min(tree.pred.seq$dev)) 
min(tree.pred.seq$size[opt.trees]) 
```

Building the tress on training dataset, we obtain a slighlty different tree pruned to 5 leaves, which splits data on grade first and then sqft_living. The plot of errors versus size also points out an optimal tree at 34 nodes.

```{r predtree, echo=FALSE}
treepr.pred <- prune.tree(tree.pred, best = 5, newdata = test.set) #evaluates on test data
treepr.pred
plot(treepr.pred) 
text(treepr.pred,cex=0.75, digits=3)
```

Evaluating this tree on test data, we can see how the trained model did a good job at predicting price for the dataset as errors and tree are almost identical.

# Random Forest

```{r rf, echo=FALSE}
ff1 <- randomForest(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data = train.set, importance = TRUE)
ff1
```

A regression random forest on all the predictors variables for price leads to 500 trees ensembled for 3 variables tried at each split, and eventually provides a 68.5% of variance explained and a MSE of 0.0875.


# PCA

```{r libpca, include=FALSE}
library("dplyr")
library("tidyr")
library("tidyverse")
library("ggplot2")
library("Rmisc")
library("MASS")
library("corrplot")
library("faraway")
library("factoextra")
library("pls")
library("magrittr")
```

```{r lod}
house <- data.frame(read.csv("kc_house_data.csv", header = TRUE))
str(house)
```

##subset data 
```{r subs}
house2 <- subset(house,select = -c(id,date,waterfront,view,zipcode,lat,long,sqft_living15,sqft_lot15,yr_renovated))
str(house2)
```
##All numeric variables. 

```{r nas}
sum(is.na(house2))
```



```{r cor}
corx=cor(house2[,-1])
corx
```



##Here we have 10 variables which are correlated and we want to reduce the number of them, pick up the important ones.

##PCA part,scale=T normalize the variables becuase they have different scales.
```{r pca}
prin_comp <- prcomp(scale(house2[,-1]))
summary(prin_comp)
prin_comp$center ##output the mean of variables
prin_comp$scale ##output the sd of variables
```

```{r rot}
prin_comp$rotation
```

```{r eig}
fviz_eig(prin_comp)
```





```{r bip}
biplot(prin_comp, scale = 0)
##The first principle component looks like sqft_above, the second looks like sqft_basement)
```

```{r bip2}
biplot(prin_comp,6:7, scale =0)
```


```{r pve}
pr.var <- (prin_comp$sdev^2)
pve <- pr.var/sum(pr.var)
plot(cumsum(pve), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
```

##7 components result in close to 95% 


##Predicting use pca

```{r predpca}
houseprice <- prin_comp$x
modHouses <- lm(house2$price ~ houseprice[,1:7])
summary(modHouses)
```

```{r fullpca}
fullmodel=lm(price~.-sqft_basement,data=house2)
summary(fullmodel)
```


```{r plotpca}
par(mfrow = c(1,2))
plot(house2$price, predict(modHouses), xlab = "actual price", ylab = "Predicted price", main = "PCR", abline(a = 0, b = 1, col = "red"))
plot(house2$price, predict(fullmodel), xlab = "actual price", ylab = "Predicted price", main = "Full model", abline(a = 0, b = 1, col = "red"))
```

##R^2 of full model is 0.618 which is higher than pcr which is 0.613
##we can see that both models underestimate the value of house price over 4e+06


# Ridge and Lasso 

```{r baselib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("glmnet")
```

```{r uzscale_fcn, include=FALSE}
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )
  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
# sample
# loadPkg("ISLR")
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
```


```{r data clean}
kc_house_data <- read.csv('kc_house_data.csv')
colnames(kc_house_data)
kc_house_data <- subset(kc_house_data, select = -c(1,2, 9, 10, 17, 18, 19, 20, 21))
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)
kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)
kc_house_data <-  drop_na(kc_house_data)
# kc_house_data$condition <- as.factor(kc_house_data$condition)
# kc_house_data$grade <- as.factor(kc_house_data$grade)
colnames(kc_house_data)
nrow(kc_house_data)
```

## The Ridge 

```{r splitridge}
loadPkg("ISLR")
house_unscale = uzscale(kc_house_data)
x=model.matrix(price~.,house_unscale)[,-1]
y=house_unscale$price
loadPkg("glmnet")
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)    # Draw plot of coefficients
colnames(house_unscale)
```

The glmnet( ) function creates 100 models, with our choice of 100 $\lambda$ values. Each model coefficients are stored in the object we named: ridge.mod  
There are 12 coefficients for each model. The 100 $\lambda$ values are chosen from 0.02 ($10^{-2}$) to $10^{10}$, essentially covering the ordinary least square model ($\lambda$ = 0), and the null/constant 
model ($\lambda$ approach infinity).


```{r ridge}
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # 0.000155
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60] 
sqrt(sum(coef(ridge.mod)[-1,60]^2))  # 0.0025
```

```{r predridge}
predict(ridge.mod,s=50,type="coefficients")[1:12,]
```


Let us split the data into training and test set, so that we can estimate test errors. The split will be used here for Ridge and later for Lasso regression. 

```{r splitrid, warning=F}
loadPkg("dplyr")
set.seed(1)
train = house_unscale %>% sample_frac(0.5)
test = house_unscale %>% setdiff(train)

x_train = model.matrix(price~., train)[,-1]
x_test = model.matrix(price~., test)[,-1]

y_train = train %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
y_test = test %>% dplyr::select(price) %>% unlist() # %>% as.numeric()
```

The test set mean-squared-error MSE is 0.55 (remember that we are using standardized scores) for $\lambda = 4$. 

```{r predrid}
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x_test)
mean((ridge.pred-y_test)^2)
```


On the other hand, for the null model ($\lambda$ approach infinity), the test MSE can be found to be 0.95. So $\lambda = 4$ reduces the variance by about half at the expense of the bias.

```{r mseridge}
mean((mean(y_train)-y_test)^2) # the test set MSE
```

We could have also used a large $\lambda$ value to find the MSE for the null model. These two methods yield essentially the same answer of 0.977.
```{r mseridge2}
ridge.pred=predict(ridge.mod,s=1e10,newx=x_test)
mean((ridge.pred-y_test)^2)
```

Now for the other extreme speical case of small $\lambda$, which is the ordinary least square (OLS) model. We can first use the ridge regression result to predict the $\lambda$ =0 case. The MSE found to be 0.368 that way. 

```{r mseridg3}
ridge.pred = predict(ridge.mod, s = 0, newx = x_test)
mean((ridge.pred - y_test)^2)
predict(ridge.mod, s = 0, type="coefficients")[1:12,]
```

We can also build the OLS model directly, and calculate the MSE.
```{r}
ols.mod = lm(price~., data = train)
summary(ols.mod)
mean(residuals(ols.mod)^2) # 0.507
```


### Use Cross-validation

There is a built-in cross-validation method with glmnet, which will select the minimal $\lambda$ value.

```{r cvridge}
# set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out)
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
```

```{r cvridpred}
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:12,]
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((ridge.pred - y_test)^2)
rsq <- 1 - sse / sst
rsq
```
The first verticle dotted line is where the lowest MSE is. The second verticle dotted line is within one standard error. The labels of above the graph shows how many non-zero coefficients in the model.

## The Lasso

The same functino glmnet( ) with alpha set to 1 will build the Lasso regression model. 

```{r lasso}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out)
```
Here, we see that the lowest MSE is when $\lambda$ appro = 0.369. It has 10 non-zero coefficients. 

```{r lassocoef} 
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x_test)
mean((lasso.pred-y_test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:12,] # Display coefficients using λ chosen by CV
lasso_coef
lasso_coef[lasso_coef!=0]
```

```{r lasso R2}
sst1 <- sum((y_test - mean(y_test))^2)
sse1 <- sum((lasso.pred - y_test)^2)
rsq1 <- 1 - sse1 / sst1
rsq1
```

```{r lmlasso}
lasLin <- lm(price ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+condition+grade+sqft_basement+yr_built+yr_renovated, data = kc_house_data)
summary(lasLin)
vif(lasLin)
```



