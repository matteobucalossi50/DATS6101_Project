---
title: "Housing Market - DT"
author: "Anti-Code Group"
date: "11/26/2019"
output:  
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

```{r base_lib, include=FALSE}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("tidyverse")
loadPkg("ggplot2")
loadPkg('ggmap')
loadPkg('rjson')
loadPkg('jsonlite')
loadPkg('leaflet')
loadPkg('grDevices')
loadPkg('scales')
loadPkg('RCurl')
loadPkg('sp')
loadPkg('geojsonio')
loadPkg('lmtest')
loadPkg("faraway")
loadPkg("corrplot")
loadPkg("modelr")
loadPkg('DT')
loadPkg('plotly')
loadPkg('rmdformats')
loadPkg("rpart") 
loadPkg("caret") 
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
loadPkg("tree") 
loadPkg('ISLR')
loadPkg('randomForest')
loadPkg('leaps')
```

## Loading Data and Clean

https://www.kaggle.com/harlfoxem/housesalesprediction/data

https://github.com/matteobucalossi50/DATS6101_Project

```{r data, echo=FALSE}
kc_house_data <- read.csv("kc_house_data.csv")
str(kc_house_data)
```

```{r clean, include=FALSE}
kc_house_data <- subset(kc_house_data, select = -c(9, 10))

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bathrooms != 0)

kc_house_data <- subset(kc_house_data, kc_house_data$bedrooms < 30)

kc_house_data <-  drop_na(kc_house_data)

kc_house_data$condition <- as.factor(kc_house_data$condition)

kc_house_data$grade <- as.factor(kc_house_data$grade)

price.ln = log(kc_house_data$price)

```

# Trees

## Regression Tree

```{r tree, echo=FALSE}
tree1 <- tree(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data)
summary(tree1)
```

The tree built on all the features used as predictors (namely excluding id, date, geographic variables and sqft_living15 and sqft_lot15) for price, presents 9 termindal nodes and a mean squared error of 0.12.

```{r plott, echo=FALSE}
plot(tree1) 
text(tree1,cex=0.75, digits=3)
```

We can see that the algorithm splits the data using grade, yr_built and sqft_living. As expected, if grade is better and sqft_living are larger, price would be higher. While if grade is lower and houses get older and smaller, price would then be likely lower.
As we used the log of price to predict price with this tree, we can see how the highest average of price in the farthest right leaf is `r exp(14.07)` dollars and the lowest average of price in the farthest left is `r exp(12.52)` dollars.

```{r tree2, echo=FALSE}
tree2 <- rpart(log(price) ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=kc_house_data, cp=.02)

par(xpd = NA, mar = rep(0.7, 4)) 
plot(tree2, compress = TRUE)
text(tree2, cex = 0.7, use.n = TRUE, fancy = TRUE, all = TRUE)
fancyRpartPlot(tree2)

rpart.plot(tree2, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```

Using the package rpart, we can build a tree that provides a similar result but from which we can observe the proportion of sample observations present in each terminal leaf (in this case 7). As expected, the right branches of the tree only contain 20% of the houses, while the largest proportion are on the left branches given their lower price.

```{r maptree, echo=FALSE}
treefit = tree(log(price) ~ long+lat,data=kc_house_data)
plot(treefit)
text(treefit, cex=0.75, digits = 3)
price.deciles = quantile(kc_house_data$price,0:10/10)
cut.prices = cut(kc_house_data$price,price.deciles,include.lowest=TRUE)
plot(kc_house_data$long,kc_house_data$lat,col=grey(10:2/11)[cut.prices],pch=20,xlab="Longitude",ylab="Latitude")
partition.tree(treefit,ordvars=c("long","lat"),add=TRUE, col = 'red')
```

## Prune tree

```{r prune, echo=FALSE}
tree1.seq <- prune.tree(tree1) # Sequence of pruned tree sizes/errors
plot(tree1.seq)  # error versus plot size
tree1.seq$dev # Vector of error rates for prunings in order 

opt.trees = which(tree1.seq$dev == min(tree1.seq$dev)) # Positions of optimal (with respect to error) trees
min(tree1.seq$size[opt.trees]) # Size of smallest optimal tree

prune.tree(tree1,best=5) # Returns best pruned tree with all data
```
Plotting the error versus the size of trees, we can see that the optimal pruned tree (smallest tree minimizing the errors) would have 9 nodes.

## Testing model 

```{r split, echo=FALSE}
fold <- floor(runif(nrow(kc_house_data),1,11)) 
  kc_house_data$fold <- fold
test.set <- kc_house_data[kc_house_data$fold == 1,] 
train.set <- kc_house_data[kc_house_data$fold != 1,] 
```

```{r test, echo=FALSE}
tree.pred <- tree(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data=train.set, mindev=0.001)
treepr <- prune.tree(tree.pred, best = 5) # Return best pruned tree with 5 leaves, evaluating error on training data
treepr
plot(treepr) 
text(treepr,cex=0.75, digits=3)

tree.pred.seq <- prune.tree(tree.pred)
plot(tree.pred.seq)
tree.pred.seq$dev
opt.trees = which(tree.pred.seq$dev == min(tree.pred.seq$dev)) 
min(tree.pred.seq$size[opt.trees]) 
```

Building the tress on training dataset, we obtain a slighlty different tree pruned to 5 leaves, which splits data on grade first and then sqft_living. The plot of errors versus size also points out an optimal tree at 34 nodes.

```{r pred, echo=FALSE}
treepr.pred <- prune.tree(tree.pred, best = 5, newdata = test.set) #evaluates on test data
treepr.pred
plot(treepr.pred) 
text(treepr.pred,cex=0.75, digits=3)
```

Evaluating this tree on test data, we can see how the trained model did a good job at predicting price for the dataset as errors and tree are almost identical.

# Random Forest

```{r rf, echo=FALSE}
ff1 <- randomForest(log(price)~bedrooms + bathrooms + sqft_living + sqft_lot + floors + condition + grade + sqft_above + sqft_basement+yr_built+yr_renovated, data = train.set, importance = TRUE)
ff1
```

A regression random forest on all the predictors variables for price leads to 500 trees ensembled for 3 variables tried at each split, and eventually provides a 68.5% of variance explained and a MSE of 0.0875.
